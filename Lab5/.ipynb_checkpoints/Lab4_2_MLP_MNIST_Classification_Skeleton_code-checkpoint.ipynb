{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> PyTorch Classification with MLP </h1>\n",
    "Lets see how we can train our first neural network using the Pytorch funcunalities we have previously seen! In this notebook we will be training a Multilayer Perceptron (MLP) with the MNIST dataset. We will see how to use Pytorch inbuilt datasets, how to construct a MLP using the Pytorch nn.module class and how to construct a training and testing loop to perform stochastic gradient descent (SGD).\n",
    "\n",
    "<img src=\"MNIST.gif\" width=\"700\" align=\"center\">\n",
    "\n",
    "Animation of MNIST digits and a MLP's activations changing via learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Download the MNIST Train and Test set </h2>\n",
    "The MNIST dataset is a large database of handwritten digits that is commonly used for training and testing in the field of machine learning, it consists of 60,000 training images and 10,000 testing images as well as the corresponding digit class (0-9) (it has moved out of fashion these days because it is \"too easy\" to learn though it is still used at times as a \"proof of concept\").  <br>\n",
    "Pytorch has constructed a number of \"dataset\" classes that will automatically download various datasets making it very easy for us to train our models. We will look more closely at using Pytorch datasets in a later lab.\n",
    "\n",
    "[Pytorch Datasets](https://pytorch.org/docs/stable/torchvision/datasets.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a train and test dataset using the Pytorch MNIST dataloader class\n",
    "train = MNIST('./data', train=True,  download=True, transform=transforms.ToTensor())\n",
    "test  = MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "#Using the Pytorch dataloader class and the Pytorch datasets we with create itterable dataloader objects\n",
    "train_loader = dataloader.DataLoader(train, shuffle=True, batch_size=256, num_workers=4, pin_memory=True) \n",
    "test_loader = dataloader.DataLoader(test, shuffle=True, batch_size=256, num_workers=4, pin_memory=True)\n",
    "\n",
    "#NOTE:num_workers is the number of threads the dataloader will spawn to load the data from file, \n",
    "#you will rarely need more than 4\n",
    "\n",
    "#NOTE:pin_memory is only useful if you are training with a GPU, if it is True then the GPU will pre-allocate\n",
    "#memory for the NEXT batch so the CPU-GPU transfer can be handled by the DMA controller freeing up the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set device to GPU_indx if GPU is avaliable\n",
    "GPU_indx = 0\n",
    "device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Visualise a few training samples </h3>\n",
    "Lets visualise that mini-batches that the dataloader gives us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can create an itterater using the dataloaders and take a random sample \n",
    "images, labels = next(iter(train_loader))\n",
    "print(\"The input data shape is :\\n\", images.shape)\n",
    "print(\"The target output data shape is :\\n\", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that (as specified) our mini-batch is 256. The dataloader has passed us a 4D Tensor as input data, the first dimension (d0) is known as the \"batch dimension\" (B) the other three are the image dimensions (CxHxW). We can this of this 4D Tensor as a stack of 256, 1 channel, 28x28 images.<br>\n",
    "The image labels are a 1D Tensor, 1 single scalar value per image (per mini-batch \"instance\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets visualise an entire batch of images!\n",
    "plt.figure(figsize = (20,10))\n",
    "out = torchvision.utils.make_grid(images, 32)\n",
    "plt.imshow(out.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our Neural Network Model \n",
    "We define our model using the torch.nn.Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a simple MLP network similar to the sine wave approximator\n",
    "class Simple_MLP(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(Simple_MLP, self).__init__()\n",
    "        #We will use 4 linear layers\n",
    "        #The input to the model is 784 (28x28 - the image size)\n",
    "        #and the should be num_classes outputs\n",
    "        self.fc1 = ##TO DO-  hidded layer size 512 ##\n",
    "        self.fc2 = ##TO DO-  hidded layer size 256 ##\n",
    "        self.fc3 = ##TO DO-  hidded layer size 128 ##\n",
    "        self.fc4 = ##TO DO-  output layer size num_classes ##\n",
    "\n",
    "    def forward(self, x):\n",
    "        #the data we pass the model is a batch of single channel images\n",
    "        #with shape BSx1x28x28 we need to flatten it to BSx784\n",
    "        #To use it in a linear layer\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        #We will use a relu activation function for this network! (F.relu())\n",
    "        x = ##TO DO layer and then activation function##\n",
    "        x = ##TO DO layer and then activation function##\n",
    "        x = ##TO DO layer and then activation function##\n",
    "        x = ##TO DO add layer##\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create the model and define the Loss and Optimizer</h3>\n",
    "Since this is a classification task, we will use Cross Entropy Loss. We define our criterion using Cross Entropy Loss \n",
    "\n",
    "[torch.nn.CrossEntropyLoss](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)\n",
    "\n",
    "Just like in the sine wave approximation, experiment with different optimizers and hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our model\n",
    "model = ##TO DO create a model with 10 classes##\n",
    "#Create our loss function\n",
    "criterion = ##TO DO add the Cross Entropy loss Function ##\n",
    "#Define our loss funcition and optimizer\n",
    "lr = ##TO DO##\n",
    "optimizer = ##TO DO##\n",
    "\n",
    "#We can print out our model structure\n",
    "print(model)\n",
    "#Note: this is only the order in which the layers were defined NOT the path of the forward pass!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create a function that will train the network for one epoch </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, loss_logger):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        #forward pass of model\n",
    "        outputs =##TO DO##\n",
    "        \n",
    "        #calculate loss\n",
    "        loss = ##TO DO##\n",
    "        \n",
    "        #Zero gradients\n",
    "        ##TO DO##\n",
    "        \n",
    "        #Backprop loss\n",
    "        ##TO DO##\n",
    "        \n",
    "        #Optimization Step\n",
    "        ##TO DO##\n",
    "        \n",
    "        loss_logger.append(loss.item())\n",
    "\n",
    "    return loss_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create a function that will evaluate our network's performance on the test set </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion, loss_logger):\n",
    "    with torch.no_grad():\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):   \n",
    "            #forward pass of model\n",
    "            outputs =##TO DO##            \n",
    "            \n",
    "            #Calculate the accuracy of the model\n",
    "            #you'll need to accumulate the accuracy over multiple steps\n",
    "            \n",
    "            ##TO DO## \n",
    "            #Number of correctly predicted outputs\n",
    "            correct_predictions += ##TO DO## \n",
    "            #total number of predictions made\n",
    "            total_predictions += ##TO DO## \n",
    "            \n",
    "            #calculate the loss\n",
    "            loss = ##TO DO## \n",
    "            loss_logger.append(loss.item())\n",
    "            \n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        return loss_logger, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model for N epochs\n",
    "We call our training and testing functions in a loop, while keeping track of the losses and accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = ##TO DO## \n",
    "train_loss = []\n",
    "test_loss  = []\n",
    "test_acc   = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epochs):\n",
    "    clear_output(True)\n",
    "    print(\"Epoch: [%d/%d]\" % (i+1, n_epochs))\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, train_loss)\n",
    "    test_loss, acc = test_model(model, test_loader, criterion, test_loss)\n",
    "    test_acc.append(acc)\n",
    "\n",
    "print(\"Final Accuracy: %.2f%%\" % acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Test Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.plot(test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
