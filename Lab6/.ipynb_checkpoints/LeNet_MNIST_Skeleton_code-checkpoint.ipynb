{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Convolutional Neural Networks</h1>\n",
    "<img src=\"https://miro.medium.com/max/4348/1*PXworfAP2IombUzBsDMg7Q.png\" width=\"750\" align=\"center\">\n",
    "\n",
    "In this lab we will be constructing and training a \"Convolutional Neural Network\" aka a neural network that contains convolution kernels with learnable parameters.<br>\n",
    "We are also going to learn a bit more about Pytorch transforms and how to create save \"checkpoints\" for our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the size of our mini batches\n",
    "batch_size     = 64\n",
    "#How many itterations of our dataset\n",
    "num_epochs     = 5\n",
    "#optimizer learning rate\n",
    "learning_rate  = 0.01\n",
    "#initialise what epoch we start from\n",
    "start_epoch    = 0\n",
    "#initialise best valid accuracy \n",
    "best_valid_acc = 0\n",
    "#where to load/save the dataset from \n",
    "data_set_root = \"data\"\n",
    "\n",
    "#start from a checkpoint or start from scratch?\n",
    "start_from_checkpoint = False\n",
    "#A directory to save our model to (will create it if it doesn't exist)\n",
    "save_dir = 'Models'\n",
    "#A name for our model!\n",
    "model_name = 'LeNet5_MNIST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set device to GPU_indx if GPU is avaliable\n",
    "GPU_indx = 0\n",
    "device = torch.device(GPU_indx if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create a transform for the input data </h3>\n",
    "As we have seen, we often wish to perform some operations on data before we pass it through our model. Such operations could be, cropping or resizing images, affine transforms and data normalizations. Pytorch's torchvision module has a large number of such \"transforms\" which can be strung together sequentially using the \"Compose\" function. <br>\n",
    "\n",
    "Pytorch's inbuilt datasets take a transform as an input and will apply this transform to the data before passing it to you! This makes preprocessing data really easy! We will see more about data preprocessing in a later lab!\n",
    "\n",
    "[torchvision.transforms](https://pytorch.org/docs/stable/torchvision/transforms.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare a composition of transforms\n",
    "#transforms.Compose will perform the transforms in order\n",
    "#NOTE: some transform only take in a PIL image, others only a Tensor\n",
    "#EG Resize and ToTensor take in a PIL Image, Normalize takes in a Tensor\n",
    "#Refer to documentation\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(32),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.1307], [0.308])])\n",
    "\n",
    "#Note: ToTensor() will scale unit8 and similar type data to a float and re-scale to 0-1\n",
    "#Note: We are normalizing with the dataset mean and std "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create the training, testing and validation data</h3>\n",
    "When training many machine learning systems it is best practice to have our TOTAL dataset split into three segments, the training set, testing set and validation set. Up until now we have only had a train/test set split and have used the test set to gauge the performance during training. Though for the most \"unbiased\" results we should really not use our test set until training is done! So if we want to evaluate our model on an \"unseen\" part of the dataset we need another split - the validation set. <br>\n",
    "Training set   - the data we train our model on\n",
    "Validation set - the data we use to gauge model performance during training\n",
    "Testing set   - the data we use to \"rate\" our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our MNIST train and test datasets\n",
    "#Can also try with CIFAR10 Dataset\n",
    "#https://pytorch.org/docs/stable/torchvision/datasets.html#mnist\n",
    "train_data = ########Fill out#########\n",
    "test_data  = ########Fill out#########\n",
    "\n",
    "#We are going to split the train dataset into a train and validation set 90/10\n",
    "validation_split = 0.9\n",
    "\n",
    "#Determine the number of samples for each split\n",
    "n_train_examples = int(len(train_data)*validation_split)\n",
    "n_valid_examples = len(train_data) - n_train_examples\n",
    "\n",
    "#The function random_split will take our dataset and split it randomly and give us dataset\n",
    "#that are the sizes we gave it\n",
    "#Note: we can split it into to more then two pieces!\n",
    "train_data, valid_data = torch.utils.data.random_split(train_data, [n_train_examples, n_valid_examples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Check the lengths of all the datasets</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of training examples: {len(train_data)}')\n",
    "print(f'Number of validation examples: {len(valid_data)}')\n",
    "print(f'Number of testing examples: {len(test_data)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create the dataloader</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the training, Validation and Evaluation/Test Datasets\n",
    "#It is best practice to separate your data into these three Datasets\n",
    "#Though depending on your task you may only need Training + Evaluation/Test or maybe only a Training set\n",
    "#(It also depends on how much data you have)\n",
    "#https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataloader\n",
    "train_loader =  ########Fill out#########\n",
    "valid_loader =  ########Fill out#########\n",
    "test_loader  =  ########Fill out#########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Create th LeNet5 network</h2>\n",
    "LeNet5 is a \"classic\" old convolution neural network (one of the oldest dating back to 1998) we will be creating an implementation of it here! It uses both convolutional layers and linear layers to \"learn\" features of the image and perform the classification. It also uses \"Max Pooling\" to downsample the \"feature maps\" (the 2d hidden layers at the output of a convolutional layer)\n",
    "\n",
    "[Max Pooling](https://computersciencewiki.org/index.php/Max-pooling_/_Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self, channels_in):\n",
    "        #Call the __init__ function of the parent nn.module class\n",
    "        super(LeNet, self).__init__()\n",
    "        #Define Convolution Layers\n",
    "        #conv1 6 channels_inx5x5 kernals\n",
    "        self.conv1 = ########Fill out#########\n",
    "        \n",
    "        #conv2 16 6x5x5 kernals\n",
    "        self.conv2 =  ########Fill out#########\n",
    "        \n",
    "        #Define MaxPooling Layers\n",
    "        #https://computersciencewiki.org/index.php/Max-pooling_/_Pooling\n",
    "        #Default Stride is = to kernel_size\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Define Linear/Fully connected/ Dense Layers\n",
    "        #Input to linear1 is the number of features from previous conv - 16x5x5\n",
    "        #output of linear1 is 120\n",
    "        self.linear1 =  ########Fill out#########\n",
    "        #output of linear2 is 84\n",
    "        self.linear2 =  ########Fill out#########\n",
    "        #output of linear3 is 10\n",
    "        self.linear3 =  ########Fill out#########\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #Pass input through conv layers\n",
    "        #x shape is BatchSize-3-32-32\n",
    "        \n",
    "        out1 = #Conv then F.relu()  ########Fill out#########\n",
    "        #out1 shape is BatchSize-6-28-28\n",
    "        out1 = #maxpool  ########Fill out#########\n",
    "        #out1 shape is BatchSize-6-14-14\n",
    "\n",
    "        out2 = #Conv then F.relu()  ########Fill out#########\n",
    "        #out2 shape is BatchSize-16-10-10\n",
    "        out2 = #maxpool  ########Fill out#########\n",
    "        #out2 shape is BatchSize-16-5-5\n",
    "\n",
    "        #Flatten out2 to shape BatchSize-16x5x5\n",
    "        out2 =  ########Fill out#########\n",
    "        \n",
    "        out3 = #linear then F.relu()  ########Fill out#########\n",
    "        #out3 shape is BatchSize-120\n",
    "        out4 = #linear then F.relu()  ########Fill out#########\n",
    "        #out4 shape is BatchSize-84\n",
    "        out5 = #linear to output  ########Fill out#########\n",
    "        #out5 shape is BatchSize-10\n",
    "        return out5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Create our model and view the ouput! </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataloader itterable object\n",
    "dataiter = iter(train_loader)\n",
    "#sample from the itterable object\n",
    "images, labels = dataiter.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance of our network\n",
    "#set channels_in to the number of channels of the dataset images\n",
    "net =  ########Fill out#########\n",
    "#view the network\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass image through network\n",
    "out =  ########Fill out#########\n",
    "#check output\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Set up the optimizer </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass our network parameters to the optimiser set our lr as the learning_rate\n",
    "#https://pytorch.org/docs/stable/optim.html#torch.optim.Adam\n",
    "optimizer = optim.Adam(########Fill out#########)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a Cross Entropy Loss\n",
    "#https://pytorch.org/docs/stable/nn.html#torch.nn.CrossEntropyLoss\n",
    "Loss_fun = ########Fill out#########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Loading Checkpoints</h3>\n",
    "This bit of code will load the parameters of a model and a optimizer from file if start_from_checkpoint == True. Saving your model parameters during training is a good idea!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Save Path from save_dir and model_name, we will save and load our checkpoint here\n",
    "Save_Path = os.path.join(save_dir, model_name + \".pt\")\n",
    "\n",
    "#Create the save directory if it does note exist\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "#Load Checkpoint\n",
    "if start_from_checkpoint:\n",
    "    #Check if checkpoint exists\n",
    "    if os.path.isfile(Save_Path):\n",
    "        #load Checkpoint\n",
    "        check_point = torch.load(Save_Path)\n",
    "        #Checkpoint is saved as a python dictionary\n",
    "        #https://www.w3schools.com/python/python_dictionaries.asp\n",
    "        #here we unpack the dictionary to get our previous training states\n",
    "        net.load_state_dict(check_point['model_state_dict'])\n",
    "        optimizer.load_state_dict(check_point['optimizer_state_dict'])\n",
    "        start_epoch = check_point['epoch']\n",
    "        best_valid_acc = check_point['valid_acc']\n",
    "        print(\"Checkpoint loaded, starting from epoch:\", start_epoch)\n",
    "    else:\n",
    "        #Raise Error if it does not exist\n",
    "        raise ValueError(\"Checkpoint Does not exist\")\n",
    "else:\n",
    "    #If checkpoint does exist and Start_From_Checkpoint = False\n",
    "    #Raise an error to prevent accidental overwriting\n",
    "    if os.path.isfile(Save_Path):\n",
    "        raise ValueError(\"Warning Checkpoint exists\")\n",
    "    else:\n",
    "        print(\"Starting from scratch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the accuracy calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(fx, y):\n",
    "    preds = fx.max(1, keepdim=True)[1]\n",
    "    correct = preds.eq(y.view_as(preds)).sum()\n",
    "    acc = correct.float()/preds.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function should perform a single training epoch using our training data\n",
    "def train(net, device, loader, optimizer, Loss_fun, loss_logger):\n",
    "    \n",
    "    #initialise counters\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    #Set Network in train mode\n",
    "    ########Fill out#########\n",
    "    \n",
    "    for i, (x, y) in enumerate(loader):\n",
    "        \n",
    "        #load images and labels to device\n",
    "        x =  # x is the image\n",
    "        y =  # y is the corresponding label\n",
    "                \n",
    "        #Forward pass of image through network and get output\n",
    "        fx = ########Fill out#########\n",
    "        \n",
    "        #Calculate loss using loss function\n",
    "        loss = ########Fill out#########\n",
    "        \n",
    "        #calculate the accuracy\n",
    "        acc = ########Fill out#########\n",
    "\n",
    "        #Zero Gradents\n",
    "        ########Fill out#########\n",
    "        \n",
    "        #Backpropagate Gradents\n",
    "        ########Fill out#########\n",
    "        \n",
    "        #Do a single optimization step\n",
    "        ########Fill out#########\n",
    "        \n",
    "        #create the cumulative sum of the loss and acc\n",
    "        epoch_loss += ########Fill out#########\n",
    "        epoch_acc += ########Fill out#########\n",
    "        \n",
    "        #log the loss for plotting\n",
    "        loss_logger.append(loss.item())\n",
    "\n",
    "        #clear_output is a handy function from the IPython.display module\n",
    "        #it simply clears the output of the running cell\n",
    "        \n",
    "        clear_output(True)\n",
    "        print(\"TRAINING: | Itteration [%d/%d] | Loss %.2f |\" %(i+1 ,len(loader) , loss.item()))\n",
    "        \n",
    "    #return the avaerage loss and acc from the epoch as well as the logger array       \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader), loss_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function should perform a single evaluation epoch and will be passed our validation or evaluation/test data\n",
    "#it WILL NOT be used to train out model\n",
    "def evaluate(net, device, loader, Loss_fun, loss_logger = None):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    #Set network in evaluation mode\n",
    "    #Layers like Dropout will be disabled\n",
    "    #Layers like Batchnorm will stop calculating running mean and standard deviation\n",
    "    #and use current stored values\n",
    "    ########Fill out#########\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (x, y) in enumerate(loader):\n",
    "            \n",
    "            #load images and labels to device\n",
    "            x = ########Fill out#########\n",
    "            y = ########Fill out#########\n",
    "            \n",
    "            #Forward pass of image through network\n",
    "            fx = ########Fill out#########\n",
    "            \n",
    "            #Calculate loss using loss function\n",
    "            loss = ########Fill out#########\n",
    "            \n",
    "            #calculate the accuracy\n",
    "            acc = ########Fill out#########\n",
    "            \n",
    "            #log the cumulative sum of the loss and acc\n",
    "            epoch_loss += ########Fill out#########\n",
    "            epoch_acc += ########Fill out#########\n",
    "            \n",
    "            #log the loss for plotting if we passed a logger to the function\n",
    "            if not (loss_logger is None):\n",
    "                loss_logger.append(loss.item())\n",
    "            \n",
    "            clear_output(True)\n",
    "            print(\"EVALUATION: | Itteration [%d/%d] | Loss %.2f | Accuracy %.2f%% |\" %(i+1 ,len(loader), loss.item(), 100*(epoch_acc/ len(loader))))\n",
    "    \n",
    "    #return the avaerage loss and acc from the epoch as well as the logger array       \n",
    "    return epoch_loss / len(loader), epoch_acc / len(loader), loss_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell implements our training loop\n",
    "training_loss_logger = []\n",
    "validation_loss_logger = []\n",
    "\n",
    "for epoch in range(start_epoch, num_epochs):\n",
    "    \n",
    "    #call the training function and pass training dataloader etc\n",
    "    train_loss, train_acc, training_loss_logger = ########Fill out#########\n",
    "    \n",
    "    #call the evaluate function and pass validation dataloader etc\n",
    "    valid_loss, valid_acc, validation_loss_logger = ########Fill out#########\n",
    "\n",
    "    #If this model has the highest performace on the validation set \n",
    "    #then save a checkpoint\n",
    "    #{} define a dictionary, each entry of the dictionary is indexed with a string\n",
    "    if (valid_acc > best_valid_acc):\n",
    "        print(\"Saving Model\")\n",
    "        torch.save({\n",
    "            'epoch':                 epoch,\n",
    "            'model_state_dict':      net.state_dict(),\n",
    "            'optimizer_state_dict':  optimizer.state_dict(), \n",
    "            'train_acc':             train_acc,\n",
    "            'valid_acc':             valid_acc,\n",
    "        }, Save_Path)\n",
    "    \n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:05.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:05.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot out the training_loss_logger and validation_loss_logger\n",
    "plt.figure(figsize = (10,10))\n",
    "train_x = np.linspace(0, num_epochs, ########Fill out#########)\n",
    "plt.plot(train_x, training_loss_logger, c = \"y\")\n",
    "valid_x = np.linspace(0, num_epochs, ########Fill out#########)\n",
    "plt.plot(valid_x, validation_loss_logger, c = \"k\")\n",
    "\n",
    "plt.title(\"LeNet\")\n",
    "plt.legend([\"Training Loss\", \"Validation Loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#call the evaluate function and pass the evaluation/test dataloader etc\n",
    "test_loss, test_acc, _ = ########Fill out#########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
