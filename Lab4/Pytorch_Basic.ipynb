{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/9/96/Pytorch_logo.png/800px-Pytorch_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Lab 4: Pytorch Basics</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction</h2>\n",
    "As we've seen we can use numpy to create single layer and even multilayer linear neural networks by calculating the gradients by hand and hard coding them and training them via GD. But what if we want to create larger and more complicated networks? What if we want to use complicated and fancy loss functions or use huge datasets and train with more complicated training regimes?! And what about training on GPUs.......<br>\n",
    "That's a lot to try and work out EVERY time we want to try something new!! Lucky for us there are a number of Deep learning frameworks that can do much of the heavy lifting for us!<br>\n",
    "For this unit we will be using Pytorch, a hugely powerful and widely used Deep Learning framework that lets us do all of the above and MORE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Importing the required libraries </h3>\n",
    "Pytorch has two main modules, torch and torchvision<br>\n",
    "torch contains most of the Deep Learning functionalities while torchvision contains many computer vision functions designed to work in hand with torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> The Pytorch Tensor </h3>\n",
    "As we've already explored the \"Tensor\" is a useful concept and is very useful in Machine Learning, however you probably noticed in Numpy that our \"Tensors\" are called \"Arrays\", but now we are in Pytorch this is no more!!<br>\n",
    "Let's do a recap of Numpy arrays and how similar they are to Pytorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create some \"Matrices\" as lists of lists  \n",
    "\n",
    "#3x3\n",
    "W = [[1, 1, 1],\n",
    "     [1.5, 1.5, 1.5],\n",
    "     [2, 2, 2]]\n",
    "\n",
    "#3x1\n",
    "x = [[6], [7], [8]]\n",
    "#3x1\n",
    "b = [[1], [1], [1]]\n",
    "\n",
    "#Variable to store output\n",
    "#3x1\n",
    "y = [[0], [0], [0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we've seen before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " [[22. ]\n",
      " [32.5]\n",
      " [43. ]]\n",
      "Output shape:\n",
      " (3, 1)\n"
     ]
    }
   ],
   "source": [
    "#We can transform our list of lists into a \"numpy array\" by using the function \"array\"\n",
    "W_np = np.array(W)\n",
    "\n",
    "x_np = np.array(x)\n",
    "\n",
    "#lets use the function \"ones\" to create an array of ones!\n",
    "b_np = np.ones((3, 1))\n",
    "\n",
    "#Lets now compute Wx + b using these numpy variables!\n",
    "output = np.matmul(W_np, x_np) + b_np\n",
    "\n",
    "#print out the result\n",
    "print(\"Output:\\n\", output)\n",
    "print(\"Output shape:\\n\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now in Pytorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      " tensor([[22.0000],\n",
      "        [32.5000],\n",
      "        [43.0000]])\n",
      "Output shape:\n",
      " torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "#We can transform our list of lists into a \"torch tensor\" by using the function \"FloatTensor\"\n",
    "#Note: here we've specified the datatype of the tensor, a 32bit \"float\" you can also just use the function \"tensor\"\n",
    "#But this will inherit the datatype of the array given, to ensure the data-types are the same\n",
    "#(and we can perform the wanted operations) we use \"FloatTensor\"\n",
    "\n",
    "W_torch = torch.FloatTensor(W)\n",
    "\n",
    "x_torch = torch.FloatTensor(x)\n",
    "\n",
    "#lets use the function \"ones\" to create an array of ones!\n",
    "b_torch = torch.ones(3, 1)\n",
    "\n",
    "#Lets now compute Wx + b using these numpy variables!\n",
    "output = torch.matmul(W_torch,x_torch) + b_torch\n",
    "\n",
    "#print out the result\n",
    "print(\"Output:\\n\", output)\n",
    "print(\"Output shape:\\n\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! Numpy and Pytorch are remarkably similar, though this is no coincidence! The creators of Pytorch did this intentionally to make it easy to transfer existing skills in Numpy (a Python library that everyone uses - has its origins back in 1995!!) to Pytorch. To aid this transfer there are even functions that can transfer Pytorch tensors to Numpy arrays and back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a random Numpy array\n",
    "np_array = np.random.random((3, 4))\n",
    "print(\"Numpy array:\\n\", np_array)\n",
    "\n",
    "#Convert to Pytorch tensor\n",
    "torch_tensor = torch.FloatTensor(np_array)\n",
    "print(\"Pytorch tensor:\\n\", torch_tensor)\n",
    "\n",
    "#Convert back to a Numpy array!\n",
    "np_array2 = torch_tensor.numpy()\n",
    "print(\"Numpy array:\\n\", np_array2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>On to Pytorch!</h2>\n",
    "Let's further explore Pytorch and it's similarities to Numpy and then see what new functionalities it brings to the table!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Basic Element-wise Operations </h3>\n",
    "Let's quickly go back over some basics using Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a 2D Tensor using torch.rand\n",
    "y = torch.rand(4, 5)\n",
    "#this will create a \"Vector\" of numbers from 0 to 1\n",
    "print(\"Our 1D Tensor:\\n\",y)\n",
    "\n",
    "#We can perform normal python scalar arithmetic on Torch tensors\n",
    "print(\"\\nScalar Multiplication:\\n\",y * 10)\n",
    "print(\"Addition and Square:\\n\",(y + 1)**2)\n",
    "print(\"Addition:\\n\",y + y)\n",
    "print(\"Addition and division:\\n\",y / (y + 1))\n",
    "\n",
    "#We can use a combination of Torch functions and normal python arithmetic\n",
    "print(\"\\nPower and square root:\\n\",torch.sqrt(y**2))\n",
    "\n",
    "#Torch tensors are objects and have functions\n",
    "print(\"\\nY -\\n Min:%.2f\\n Max:%.2f\\n Standard Deviation:%.2f\\n Sum:%.2f\" %(y.min(), y.max(), y.std(), y.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Tensor Opperations</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create two 3D Tensors\n",
    "tensor_1 = torch.rand(3,3,3)\n",
    "tensor_2 = torch.rand(3,3,3)\n",
    "\n",
    "#Add the 2 Tensors\n",
    "print(\"Addition:\\n\",tensor_1 + tensor_2)\n",
    "\n",
    "#We cannot perform a normal \"matrix\" multiplication on a 3D tensor\n",
    "#But we can treat the 3D tensor as a \"batch\" (like a stack) of 2D tensors\n",
    "#And perform normal matrix multiplication independantly on each pair of 2D matricies\n",
    "print(\"Batch Multiplication:\\n\",torch.bmm(tensor_1,tensor_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create a more interesting tensor\n",
    "tensor_3 = torch.rand(2,4,5)\n",
    "#We can swap the Tensor dimensions\n",
    "print(\"\\nThe origional Tensor is is:\\n\", tensor_3)\n",
    "print(\"With shape:\\n\", tensor_3.shape)\n",
    "\n",
    "#tranpose will swap the dimensions it is given\n",
    "print(\"The Re-arranged is:\\n\", tensor_3.transpose(0,2))\n",
    "print(\"With shape:\\n\", tensor_3.transpose(0,2).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Indexing </h3>\n",
    "Indexing in Pytorch works the same as it does in Numpy, see if you can predict what values will be return by the indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Tensor:\n",
      " tensor([[[[0.2851, 0.2097, 0.0510, 0.3854]],\n",
      "\n",
      "         [[0.2761, 0.9551, 0.0627, 0.6965]],\n",
      "\n",
      "         [[0.1869, 0.8822, 0.9570, 0.0995]]],\n",
      "\n",
      "\n",
      "        [[[0.6571, 0.8123, 0.5152, 0.3302]],\n",
      "\n",
      "         [[0.4748, 0.7013, 0.8448, 0.2806]],\n",
      "\n",
      "         [[0.6839, 0.4991, 0.8183, 0.0602]]]])\n",
      "\n",
      "The last element of dim0:\n",
      " tensor([[[0.6571, 0.8123, 0.5152, 0.3302]],\n",
      "\n",
      "        [[0.4748, 0.7013, 0.8448, 0.2806]],\n",
      "\n",
      "        [[0.6839, 0.4991, 0.8183, 0.0602]]])\n",
      "\n",
      "Indexed elements:\n",
      " tensor([[0.2761, 0.9551, 0.0627, 0.6965]])\n",
      "\n",
      "Indexed elements:\n",
      " tensor([0.0627, 0.8448])\n"
     ]
    }
   ],
   "source": [
    "#Create a 4D Tensor\n",
    "tensor = torch.rand(2,3,1,4)\n",
    "print(\"Our Tensor:\\n\",tensor)\n",
    "\n",
    "#Select the last element of dim0\n",
    "print(\"\\nThe last element of dim0:\\n\",tensor[-1])\n",
    "\n",
    "#1st element of dim0\n",
    "#2nd element of dim1\n",
    "print(\"\\nIndexed elements:\\n\",tensor[0, 1])\n",
    "\n",
    "#Select all elements of dim0\n",
    "#The 2nd element of dim1\n",
    "#The 1st element of dim2\n",
    "#The 3rd element of dim3\n",
    "print(\"\\nIndexed elements:\\n\",tensor[:, 1, 0, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Describing Tensors </h3> <br>\n",
    "Lets see how we can view the characteristics of our Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a large 4D Tensor\n",
    "tensor = torch.rand(3, 5, 3, 2)\n",
    "\n",
    "#View the Number of elements in every dimension\n",
    "print(\"The Tensors shape is:\", tensor.shape)\n",
    "\n",
    "#In Pytorch shape and size() to the same thing!\n",
    "print(\"The Tensors shape using size() is:\", tensor.size())\n",
    "\n",
    "#View the number of elements in total\n",
    "print(\"There are %d elements in total:\" % tensor.numel())\n",
    "\n",
    "#View the number of Dimensions(2 in this case)\n",
    "print(\"There are %d Dimensions\" %(tensor.ndim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Reshaping </h3> <br>\n",
    "We can change a Tensor to one of the same size (same number of elements) but a different shape by using functions in a similar fashion to Numpy but with different functions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us reshape our Tensor to a 2D Tensor\n",
    "print(\"Reshape to 3x30:\\n\", tensor.view(3,30))\n",
    "\n",
    "#We can also use the Flatten method to convert to a 1D Tensor\n",
    "print(\"Flatten to a 1D Tensor:\\n\",tensor.flatten())\n",
    "\n",
    "#Here the -1 tells Pytorch to put as many elements as it needs here in order to maintain the given dimention sizes\n",
    "#AKA \"I don't care the size of this dimention as long as the first one is 10\"\n",
    "print(\"Reshape to 10xwhatever:\\n\",tensor.view(10,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Squeezing and Unsqueezing </h4>\n",
    "A very common shape-changing operation is to add an \"empty\" dimension to ensure the shape (specifically the number of dimensions) of the tensor is correct for certain functions. <br>\n",
    "For example, when we start using Pytorch Neural Network modules, we need to provide the input of the network with a \"batch\" dimension (we often pass multiple inputs to our network at once) even if we only pass 1 datapoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a 2D Tensor\n",
    "tensor = torch.rand(3, 2)\n",
    "\n",
    "#View the Number of elements in every dimension\n",
    "print(\"The Tensors shape is:\", tensor.shape)\n",
    "\n",
    "#unsqueeze adds an \"empty\" dimension to our Tensor\n",
    "print(\"Add an empty dimenson to dim3:\", tensor.unsqueeze(2).shape)\n",
    "\n",
    "#unsqueeze adds an \"empty\" dimension to our Tensor\n",
    "print(\"Add an empty dimenson to dim2:\", tensor.unsqueeze(1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets create a 4D Tensor with a few \"empty\" dimensions\n",
    "tensor = torch.rand(1, 3, 1, 2)\n",
    "\n",
    "#View the Number of elements in every dimension\n",
    "print(\"The Tensors shape is:\", tensor.shape)\n",
    "\n",
    "#squeeze removes an \"empty\" dimension from our Tensor\n",
    "print(\"Remove empty dimenson dim3:\", tensor.squeeze(2).shape)\n",
    "\n",
    "#squeeze removes an \"empty\" dimension from our Tensor\n",
    "print(\"Remove empty dimenson dim0:\", tensor.squeeze(0).shape)\n",
    "\n",
    "#If we don't specify a dimension, squeeze will remove ALL empty dimensions\n",
    "print(\"Remove all empty dimensons:\", tensor.squeeze().shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Broadcasting </h2>\n",
    "Broadcasting also works in Pytorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor 1 shape:\n",
      " torch.Size([1, 4, 3, 1])\n",
      "Tensor 2 shape:\n",
      " torch.Size([3, 4, 1, 4])\n",
      "The resulting shape is:\n",
      " torch.Size([3, 4, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "#Lets create 2 differently shaped 2D Tensors (Matrices)\n",
    "tensor1 = torch.rand(1, 4, 3, 1)\n",
    "tensor2 = torch.rand(3, 4, 1, 4)\n",
    "\n",
    "print(\"Tensor 1 shape:\\n\", tensor1.shape)\n",
    "print(\"Tensor 2 shape:\\n\", tensor2.shape)\n",
    "\n",
    "tensor3 = tensor1 + tensor2\n",
    "\n",
    "print(\"The resulting shape is:\\n\", tensor3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Pytorch Autograd </h2>\n",
    "<h4>Lets see Numpy do this!</h4>\n",
    "Now on to something that makes Pytorch (and other Deep Learning frameworks) unique, the auto-differentiable computational graphs! (don't worry about how this exactly works)<br>\n",
    "Remember how we compute the gradients of parameters (weights) of a model by \"backpropagation\". First we calculate the \"gradient\" of the loss with respect to the model's output and then using the chain rule find the gradient of the loss with respect to the parameters or the input and on and on for larger networks. Seems like a pretty repetitive process governed by some well known rules right? Well you know what is good at doing repetitive well defined things?!?! Computers!!<br>\n",
    "This \"automatic\" backpropagation (among other things) is what Pytorch REALLY gives us that makes training Neural Networks easy. So how does it do it? Well first Pytorch keeps track of everything we do!! (unless we tell it not to) It does this by forming a \"computational graph\" - a tree-like structure of all the operations we perform starting at some initial tensor. When tell Pytorch to backpropagate from some point, it works backwards up this tree and calculates and stores the gradients with respect to the point from where we back propagated from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see an example of this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Gradients\n",
      "dy/dx 2.0\n",
      "dy/dw 4.0\n",
      "dy/db 1.0\n"
     ]
    }
   ],
   "source": [
    "#lets create some tensors, requires_grad tells Pytorch we want to store the gradients for this tensor\n",
    "x = torch.FloatTensor([4])\n",
    "x.requires_grad = True\n",
    "w = torch.FloatTensor([2])\n",
    "w.requires_grad = True\n",
    "b = torch.FloatTensor([3])\n",
    "b.requires_grad = True\n",
    "\n",
    "#By performing a computation Pytorch will build a computational graph.\n",
    "y = w * x + b    # y = 2 * x + 3\n",
    "\n",
    "#It's easy to see that\n",
    "#dy/dx = w\n",
    "#dy/dw = x\n",
    "#dy/db = 1\n",
    "\n",
    "#Compute gradients via Pytorch's Autograd\n",
    "y.backward()\n",
    "\n",
    "#Print out the calculated gradients\n",
    "#These gradients are the gradients with respect to the point where we backprop'd from - y\n",
    "print(\"Calculated Gradients\")    # x.grad = dy/dx = 2 \n",
    "print(\"dy/dx\", x.grad.item())    # x.grad = dy/dx = 2 \n",
    "print(\"dy/dw\", w.grad.item())    # w.grad = dy/dw = 4\n",
    "print(\"dy/db\", b.grad.item())   # b.grad = dy/db = 1  \n",
    "#Note: .item() simply returns a 0D Tensor as a Python scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do something a little more interesting and introduce a Pytorch \"Linear layer\"<br>\n",
    "This \"Linear layer\" is one of the basic building blocks of neural networks. A single layer is akin to a linear regression model's $\\theta$ parameters. The Pytorch \"Linear'' class however, is a bit more complicated than just a Tensor of parameters. It is in fact an instance of a Pytorch Neural Network class (nn.module). By default it's Tensor of parameters has requires_grad=True, it also has functionality that allow the \"backwards\" pass of the model, required by Pytorch's Autograd. Finally, as we'll see when we make our own network, it has a function that is called automatically when we pass data to this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:\n",
      " tensor([[-0.7238, -1.8526,  0.3021],\n",
      "        [ 0.4872, -0.4817, -0.3063],\n",
      "        [-0.6852,  1.2065, -0.1375],\n",
      "        [-1.0599,  1.0035,  1.8523],\n",
      "        [-0.9994, -0.6717, -0.6844],\n",
      "        [-0.5757,  0.1258,  0.2069],\n",
      "        [ 0.9757,  1.2868, -0.4972],\n",
      "        [-1.0397,  0.8363,  0.4712],\n",
      "        [-0.2310, -0.3825, -0.7933],\n",
      "        [-0.2247, -1.1272,  0.4225]])\n",
      "Output data:\n",
      " tensor([[ 1.0236e+00,  9.2055e-01],\n",
      "        [ 1.5576e-01, -8.6465e-01],\n",
      "        [-6.0999e-01, -1.4656e+00],\n",
      "        [ 4.7489e-01, -7.9874e-01],\n",
      "        [-1.3336e+00,  4.3276e-01],\n",
      "        [-6.1701e-01, -1.1614e+00],\n",
      "        [ 4.7162e-01, -7.5106e-01],\n",
      "        [-1.4227e+00,  1.1371e-01],\n",
      "        [-1.1863e+00,  3.3933e-01],\n",
      "        [ 4.9271e-01,  3.8111e-05]])\n"
     ]
    }
   ],
   "source": [
    "# Create some random data tensors of shape (10, 3) and (10, 2).\n",
    "data = torch.randn(10, 3)\n",
    "target = torch.randn(10, 2)\n",
    "print ('Input data:\\n', data)\n",
    "print ('Output data:\\n', target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " tensor([[ 0.3592,  0.4478,  0.1253],\n",
      "        [ 0.0270, -0.1839, -0.5088]])\n",
      "b:\n",
      " tensor([0.1097, 0.5425])\n"
     ]
    }
   ],
   "source": [
    "#Build a linear layer (aka a \"fully connected\" layer or a \"Perceptron\" layer)\n",
    "#nn.Linear(Number of inputs, Number of outputs) \n",
    "linear = nn.Linear(3, 2) \n",
    "\n",
    "#Lets have a look at the parameters of this layer\n",
    "#The \"weights\" are what is multipied by the input data\n",
    "print ('w:\\n', linear.weight.data)\n",
    "#The bias is then added on!\n",
    "print ('b:\\n', linear.bias.data)\n",
    "#Note: Pytorch's Linear Layer includes a bias term by default!\n",
    "#Note: .data just gives us the raw Tensor without any connectionm to the computational graph\n",
    "#- it looks nicer when we print it out\n",
    "#Note: The opperation the linear layer performs is y = x*A^t + b\n",
    "#where A^t is the transpose of the weights and b is the bias, this is also know as an \"affine transformation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something to note is that Pytorch initialises the grad of the tensors to \"None\" NOT 0! They only get created after the first backwards pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w:\n",
      " None\n",
      "b:\n",
      " None\n"
     ]
    }
   ],
   "source": [
    "#Lets have a look at the gradients of these parameters\n",
    "print ('w:\\n', linear.weight.grad)\n",
    "print ('b:\\n', linear.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets introduce a few more useful functions of Pytorch! (I hope you're still with us!)<br>\n",
    "<b>Loss functions</b><br>\n",
    "We've already seen loss function's before and defined our own, but using Pytorch we can pick from some predefined functions (we can also just create our own) <br>\n",
    "<b>Optimizers</b><br>\n",
    "This is the thing that will be doing the parameter updates. Pytorch has a number of different optimizers, some of which we will explore in future labs. For now we will just use our well known GD.<br>\n",
    "Note: Most optimizers are just some variant of GD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets perform a regression with a mean square error loss\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "#Lets create a Stochastic gradient descent optimizer with a learning rate of 0.01\n",
    "#(the way we will be using it, it is just normal GD) \n",
    "#When we create the optimizer we need to tell it WHAT it needs to optimize, so the first thing \n",
    "#We pass it are the linear layer's \"parameters\"\n",
    "optimizer = torch.optim.SGD(linear.parameters(),lr=0.01) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that everything is set up, lets perform a \"forward pass\" of our model, aka let's put the data into the model and see what comes out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to perform a forward pass of our model, we just need to \"call\" our network\n",
    "#Pytorch's nn.Module class will automatically pass it to the \"forward\" function in the layer class\n",
    "#more on this later\n",
    "target_pred = linear(data)\n",
    "print(\"Network output\\n\", target_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the pred_target is NOT the same as our random target data, let's see what the MSE loss is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss_function(target_pred, target)\n",
    "print('loss:', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets perform a backward pass of our model to compute the gradients!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward pass.\n",
    "loss.backward()\n",
    "# Print out the gradients.\n",
    "print ('dL/dw: ', linear.weight.grad) \n",
    "print ('dL/db: ', linear.bias.grad)\n",
    "#Note for every backwards pass of the model we must first perform an optomization step\n",
    "#as data from parts of the computational graph have been removed upon backwards pass to save data \n",
    "#Though we can tell Pytorch to hold onto this data, in many cases it needs to be recalculated anyway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, finally, tell the optimizer to perform an update step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# he critical step to update the parameter which reduce the loss\n",
    "optimizer.step()\n",
    "\n",
    "#Perform another forward pass of the model to check the new loss\n",
    "target_pred = linear(data)\n",
    "loss = loss_function(target_pred, target)\n",
    "print('loss after 1 step optimization: ', loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our loss has gone down!! As this data is random and the model is small, it probably won't reach 0. Lets see how low we can get it to go by constructing a training loop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets create an empty array to log the loss\n",
    "loss_logger = []\n",
    "\n",
    "#Lets perform 100 itterations of our dataset\n",
    "for i in range(1000):\n",
    "    #Perform a forward pass of our data\n",
    "    target_pred = linear(data)\n",
    "    \n",
    "    #Calculate the loss\n",
    "    loss = loss_function(target_pred, target)\n",
    "    \n",
    "    #.zero_grad sets the stored gradients to 0\n",
    "    #If we didn't do this they would be added to the \n",
    "    #Gradients from the previous step!\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    #Calculate the new gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    #Perform an optimization step!\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_logger.append(loss.item())\n",
    "    \n",
    "    \n",
    "print(\"loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets graph out the loss!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wohoo! We trained our first Pytorch neural network!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
